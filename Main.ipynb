{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "The rapid growth of urbanization and industrial production has significantly impacted the environment, leading to an alarming increase in waste generation. Managing this waste effectively has become a global priority. **Recycling**, a key solution to this challenge, transforms waste into valuable resources through proper sorting and processing. This project focuses on enabling automated systems, such as smart waste-sorting robots and intelligent waste bins, to classify waste accurately by analyzing trash images.\n",
    "\n",
    "This tutorial outlines the step-by-step process of training the lightweight Convolutional Neural Network (CNN) **EfficientNet-B0** on a custom dataset of 2,527 images representing different types of trash. The resulting model serves as a foundation for lightweight waste management image classification systems, designed for real-world applications such as live deployment in smart bins.\n",
    "\n",
    "---\n",
    "\n",
    "#### About EfficientNet\n",
    "\n",
    "**EfficientNet** is a family of models (B0–B7) that balance computational efficiency and accuracy. Higher-numbered models require more computational resources and larger datasets, while lower-numbered models are lightweight and suitable for resource-constrained environments. \n",
    "\n",
    "For this project, we selected the **EfficientNet-B0** CNN model due to its efficiency, ease of training, and minimal computational demands. This lightweight model is ideal for deployment in distributed systems, such as smart bins, where compactness and low-power requirements are critical.\n",
    "\n",
    "---\n",
    "\n",
    "#### About Our Data\n",
    "\n",
    "Our dataset is imported from the Kaggle directory of **Gary Huang and Mindy Yang's** TrashNet repository (FeyZazkefe on Kaggle). It contains 2,527 images of trash, distributed across six categories:\n",
    "\n",
    "1. **Cardboard**: 403 images\n",
    "2. **Glass**: 501 images\n",
    "3. **Metal**: 410 images\n",
    "4. **Paper**: 594 images\n",
    "5. **Plastic**: 482 images\n",
    "6. **Trash**: 137 images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Import Training Image Dataset & Initiate Code Carbon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 00:05:08] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 00:05:08] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 00:05:08] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 00:05:08] No GPU found.\n",
      "[codecarbon INFO @ 00:05:08] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 00:05:08] No CPU tracking mode found. Falling back on CPU constant mode. \n",
      " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
      "\n",
      "[codecarbon WARNING @ 00:05:09] We saw that you have a AMD Ryzen 5 7520U with Radeon Graphics but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 00:05:09] CPU Model on constant consumption mode: AMD Ryzen 5 7520U with Radeon Graphics\n",
      "[codecarbon INFO @ 00:05:09] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 00:05:09]   Platform system: Linux-6.8.0-49-generic-x86_64-with-glibc2.39\n",
      "[codecarbon INFO @ 00:05:09]   Python version: 3.10.15\n",
      "[codecarbon INFO @ 00:05:09]   CodeCarbon version: 2.8.1\n",
      "[codecarbon INFO @ 00:05:09]   Available RAM : 14.861 GB\n",
      "[codecarbon INFO @ 00:05:09]   CPU count: 8\n",
      "[codecarbon INFO @ 00:05:09]   CPU model: AMD Ryzen 5 7520U with Radeon Graphics\n",
      "[codecarbon INFO @ 00:05:09]   GPU count: None\n",
      "[codecarbon INFO @ 00:05:09]   GPU model: None\n",
      "[codecarbon INFO @ 00:05:12] Saving emissions data to file /home/platinumfish/Desktop/Coding_projects/Trash-classification-5550/emissions.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nimport kagglehub\\n\\n# Downloaded TrashNet dataset from Kaggle: https://www.kaggle.com/datasets/feyzazkefe/trashnet\\n# Dataset by feyzazkefe, originally sourced from Stanford TrashNet.\\n\\ntarget_path = \"./data\"\\npath = kagglehub.dataset_download(\"feyzazkefe/trashnet\", path=target_path)\\nprint(\"Path to dataset files:\", path)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 00:05:27] Energy consumed for RAM : 0.000023 kWh. RAM Power : 5.572976589202881 W\n",
      "[codecarbon INFO @ 00:05:27] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:05:27] 0.000200 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "# Start CodeCarbon tracking\n",
    "tracker = EmissionsTracker(allow_multiple_runs=True)\n",
    "tracker.start()\n",
    "\n",
    "\"\"\"\n",
    "import kagglehub\n",
    "\n",
    "# Downloaded TrashNet dataset from Kaggle: https://www.kaggle.com/datasets/feyzazkefe/trashnet\n",
    "# Dataset by feyzazkefe, originally sourced from Stanford TrashNet.\n",
    "\n",
    "target_path = \"./data\"\n",
    "path = kagglehub.dataset_download(\"feyzazkefe/trashnet\", path=target_path)\n",
    "print(\"Path to dataset files:\", path)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Split the Imported Image Data into Training, Validation, and Test Sets\n",
    "\n",
    "After replacing **`original_data_dir`** with the correct directory, running the code below will produce the following splits of the dataset:\n",
    "\n",
    "1. **Training Images**: 1815 images used to train the model.\n",
    "2. **Test Images**: 508 images reserved for evaluating the model’s performance after training.\n",
    "3. **Validation Images**: 204 images used to tune the model during training and prevent overfitting.\n",
    "\n",
    "This split ensures a balanced dataset division, optimizing the model’s training and evaluation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "original_data_dir = \"data/trashnet\" #replace with wherever the original image data folder is\n",
    "split_data_dir = \"data/split_data\"\n",
    "categories = [\"cardboard\", \"glass\", \"metal\", \"paper\", \"plastic\", \"trash\"]\n",
    "\n",
    "# Create directories for train, val, and test splits\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    for category in categories:\n",
    "        os.makedirs(os.path.join(split_data_dir, split, category), exist_ok=True)\n",
    "\n",
    "# Split data for each category\n",
    "for category in categories:\n",
    "    category_path = os.path.join(original_data_dir, category)\n",
    "    images = os.listdir(category_path)\n",
    "    images = [img for img in images if img.endswith(('.jpg', '.png'))]  # Filter image files\n",
    "\n",
    "    # Split into train+val and test (80-20)\n",
    "    train_val, test = train_test_split(images, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Further split train+val into train and val (90-10 of train+val)\n",
    "    train, val = train_test_split(train_val, test_size=0.1, random_state=42)\n",
    "\n",
    "    # Copy files to split_data directory\n",
    "    for split, split_images in zip([\"train\", \"val\", \"test\"], [train, val, test]):\n",
    "        for img in split_images:\n",
    "            src_path = os.path.join(category_path, img)\n",
    "            dest_path = os.path.join(split_data_dir, split, category, img)\n",
    "            shutil.copy(src_path, dest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Define Transformation Functions for Training and Testing Image Sets\n",
    "\n",
    "To prepare our images for input into the EfficientNet-B0 model, we define specific transformation pipelines for both the training and testing datasets. These transformations ensure that the images are in the correct format, augmented for diversity, and normalized for compatibility with the pre-trained model.\n",
    "\n",
    "- **Standardization**: EfficientNet-B0 requires input images with dimensions of **(224, 224)** (height × width). This resizing is applied uniformly to both the training and testing datasets.\n",
    "- **Data Augmentation (Training Only)**: To enhance the diversity of the training data, random augmentations are applied:\n",
    "  - **Random Horizontal Flips**: Introduce variability by flipping images horizontally with a certain probability.\n",
    "  - **Random Rotations**: Images are randomly rotated up to ±10 degrees to simulate different orientations.\n",
    "- **Normalization**: Both the training and testing datasets are normalized to align with the ImageNet normalization method used during EfficientNet's pre-training:\n",
    "  - **Mean**: [0.485, 0.456, 0.406] (per channel for RGB images).\n",
    "  - **Standard Deviation**: [0.229, 0.224, 0.225].\n",
    "\n",
    "These transformations ensure that the training data is varied and robust while maintaining consistency for the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to EfficientNet input size\n",
    "    transforms.RandomHorizontalFlip(),  # Lightweight and effective\n",
    "    transforms.RandomRotation(10),  # Augment slightly with small angles\n",
    "    transforms.ToTensor(),  # Convert image to PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to EfficientNet input size\n",
    "    transforms.ToTensor(),  # Convert image to PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_dataset = ImageFolder(root=\"data/split_data/train\", transform=transform_train)\n",
    "val_dataset = ImageFolder(root=\"data/split_data/val\", transform=transform_test)\n",
    "test_dataset = ImageFolder(root=\"data/split_data/test\", transform=transform_test)\n",
    "\n",
    "# Define data loaders & batch size\n",
    "batch_size = 32 # Can also be 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Load the EfficientNet-B0 Model, Define the Number of Output Classes, and Enable GPU Usage (if Available)\n",
    "\n",
    "First, we load the EfficientNet-B0 model with **pre-trained weights**. Pre-trained weights refer to the parameters of a neural network that have already been trained on a large dataset (e.g., ImageNet). This allows us to leverage a model that has learned general image features, such as edges, textures, and shapes, without needing to train it from scratch.\n",
    "\n",
    "- **Initialize Pretrained EfficientNet-B0**: The model is initialized with these pre-learned parameters.\n",
    "- **Set Output Classes**: We specify the number of output classes for our classification task. In this case, the dataset contains **6 classes** (e.g., cardboard, glass, metal, paper, plastic, trash).\n",
    "- **Enable GPU Usage**: The code checks if a GPU is available. If so, the model is moved to the GPU to take advantage of faster computations during training and evaluation.\n",
    "\n",
    "This step ensures the model is ready for training with the appropriate number of output classes and optimized for available hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 00:06:12] Energy consumed for RAM : 0.000093 kWh. RAM Power : 5.572976589202881 W\n",
      "[codecarbon INFO @ 00:06:12] Energy consumed for all CPUs : 0.000708 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:06:12] 0.000801 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "# Load in efficientnet_b0\n",
    "weights = EfficientNet_B0_Weights.IMAGENET1K_V1\n",
    "model = efficientnet_b0(weights=weights)\n",
    "\n",
    "# Define class number\n",
    "num_classes = 6\n",
    "model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "\n",
    "# Print class names from training data\n",
    "print(\"Classes:\", train_dataset.classes)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Define the Loss Function, Optimizer, and Training / Validation / Testing Pipeline\n",
    "\n",
    "In this step, we define the loss function, optimizer, and a robust pipeline for training, validating, and testing the model. These components are critical for optimizing the model’s performance and ensuring its ability to generalize to unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Components**\n",
    "\n",
    "- **Class Weights**: To address class imbalances in the dataset, class weights are computed using the distribution of training samples. These weights ensure that underrepresented classes are given more importance during training by penalizing their misclassification.\n",
    "- **Loss Function**: The `nn.CrossEntropyLoss` function, combined with class weights, is used to handle multi-class classification and mitigate the effects of class imbalance.\n",
    "- **Optimizer**: The Adam optimizer is chosen for its efficiency and ability to handle sparse gradients. A learning rate of `0.0001` is used to enable stable and precise updates.\n",
    "\n",
    "---\n",
    "\n",
    "### **Training Pipeline**\n",
    "\n",
    "- **`train_model_with_early_stopping` Function**:\n",
    "  - Trains the model for a specified number of epochs (default: 20) while incorporating early stopping to prevent overfitting.\n",
    "  - Early stopping monitors validation loss and halts training if there is no improvement for a defined patience period (default: 3 epochs).\n",
    "  - The best model (with the lowest validation loss) is saved to the `Model_save` directory.\n",
    "  - During each epoch:\n",
    "    1. The model processes the training data in batches, computes the loss, and updates weights via backpropagation.\n",
    "    2. Training loss and accuracy are calculated and printed for every epoch.\n",
    "    3. Validation is performed after each epoch using the `validate_model_with_loss` function.\n",
    "\n",
    "- **`validate_model_with_loss` Function**:\n",
    "  - This function evaluates the model on the validation dataset in evaluation mode (`model.eval()`), ensuring layers like dropout and batch normalization behave correctly.\n",
    "  - Calculates validation loss and accuracy to monitor performance across epochs.\n",
    "  - The validation loss is used for the early stopping mechanism.\n",
    "\n",
    "---\n",
    "\n",
    "### **Testing with Detailed Metrics**\n",
    "\n",
    "- **`validate_model_with_metrics_and_report` Function**:\n",
    "  - After training, the model is evaluated on the test dataset to assess its generalization ability. This function computes detailed performance metrics, including:\n",
    "    - **Precision, Recall, and F1 Score**: Measures how well the model identifies each class and balances false positives and false negatives.\n",
    "    - **AUC (Area Under Curve)**: Indicates the model's ability to distinguish between classes, calculated using a one-vs-rest approach.\n",
    "  - Produces a **classification report** summarizing the performance of the model for each class.\n",
    "  - Outputs metrics such as precision, recall, F1 score, and AUC, offering a comprehensive evaluation of the model.\n",
    "\n",
    "---\n",
    "\n",
    "### **Improvements in the Pipeline**\n",
    "\n",
    "1. **Handles Class Imbalances**: Class weights are calculated dynamically based on training data distribution, ensuring balanced learning.\n",
    "2. **Early Stopping**: Prevents overfitting by halting training when the validation loss stops improving.\n",
    "3. **Detailed Testing Metrics**: Incorporates precision, recall, F1 score, and AUC to provide a deeper understanding of the model’s performance beyond accuracy.\n",
    "4. **Model Checkpointing**: Automatically saves the best model (based on validation loss) for final evaluation and deployment.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_162476/2146259924.py:89: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('Model_save/Efficientnet_trash_classifier_final.pth'))  # Load the weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 00:06:27] Energy consumed for RAM : 0.000116 kWh. RAM Power : 5.572976589202881 W\n",
      "[codecarbon INFO @ 00:06:27] Energy consumed for all CPUs : 0.000885 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:06:27] 0.001001 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "# Define loss function with class weights and optimizer\n",
    "class_counts = Counter(train_dataset.targets)\n",
    "class_weights = torch.tensor([1.0 / count for count in class_counts.values()], device=device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "def train_model_with_early_stopping(model, train_loader, val_loader, criterion, optimizer, epochs=20, patience=3):\n",
    "    # Create directory for saving the model if it doesn't exist\n",
    "    model_dir = 'Model_save'\n",
    "    model_path = os.path.join(model_dir, 'Efficientnet_trash_classifier_final.pth')\n",
    "\n",
    "    best_val_loss = float('inf')  # Initialize the best validation loss as infinity\n",
    "    patience_counter = 0  # Count epochs with no improvement\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Training loop\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Zero the parameter gradients\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Calculate loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")\n",
    "\n",
    "        # Validation step\n",
    "        val_loss, val_acc = validate_model_with_loss(model, val_loader, criterion)\n",
    "\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), model_path)  # Save the best model\n",
    "            print(f\"Validation loss improved. Saving the model to {model_path}.\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement in validation loss. Patience counter: {patience_counter}/{patience}\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered. Stopping training.\")\n",
    "            break\n",
    "\n",
    "    # Load the best model\n",
    "    print(f\"Loading the best model from {model_path}.\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    return model\n",
    "\n",
    "# Helper function to validate the model and calculate loss\n",
    "def validate_model_with_loss(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    val_loss = running_loss / len(val_loader)\n",
    "    val_acc = correct / total\n",
    "    return val_loss, val_acc\n",
    "\n",
    "# Evaluate on the test set\n",
    "model = efficientnet_b0(num_classes=6)  # Adjust num_classes as per your task\n",
    "model.load_state_dict(torch.load('Model_save/Efficientnet_trash_classifier_final.pth'))  # Load the weights\n",
    "model.to(device)  # Move to the appropriate device (CPU/GPU)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Updated validation function to include classification report\n",
    "def validate_model_with_metrics_and_report(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            probs = torch.nn.functional.softmax(outputs, dim=1)  # Convert logits to probabilities\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    # F1 Score, Precision, Recall\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    # AUC (one-vs-rest)\n",
    "    auc = roc_auc_score(all_labels, all_probs, multi_class='ovr')\n",
    "\n",
    "    # Classification report\n",
    "    class_report = classification_report(all_labels, all_preds, target_names=test_loader.dataset.classes)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(class_report)\n",
    "\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "\n",
    "    return precision, recall, f1, auc, class_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Function to train our model\\ndef train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10):\\n    model.train()\\n    for epoch in range(epochs):\\n        running_loss = 0.0\\n        correct = 0\\n        total = 0\\n\\n        for inputs, labels in train_loader:\\n            inputs, labels = inputs.to(device), labels.to(device)\\n\\n            optimizer.zero_grad()  # Zero the parameter gradients\\n            outputs = model(inputs)  # Forward pass\\n            loss = criterion(outputs, labels)  # Calculate loss\\n            loss.backward()  # Backward pass\\n            optimizer.step()  # Update weights\\n\\n            # Statistics\\n            running_loss += loss.item()\\n            _, predicted = torch.max(outputs, 1)\\n            correct += (predicted == labels).sum().item()\\n            total += labels.size(0)\\n\\n        epoch_loss = running_loss / len(train_loader)\\n        epoch_acc = correct / total\\n\\n        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")\\n\\n        # Validate after each epoch\\n        validate_model(model, val_loader)\\n\\n# Function to validate our model\\ndef validate_model(model, val_loader):\\n    model.eval()\\n    all_preds = []\\n    all_labels = []\\n    with torch.no_grad():\\n        for inputs, labels in val_loader:\\n            inputs, labels = inputs.to(device), labels.to(device)\\n            outputs = model(inputs)\\n            _, predicted = torch.max(outputs, 1)\\n            all_preds.extend(predicted.cpu().numpy())\\n            all_labels.extend(labels.cpu().numpy())\\n\\n    # Generate a classification report\\n    print(classification_report(all_labels, all_preds, target_names=train_dataset.classes))\\n    '"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Function to train our model\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Zero the parameter gradients\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Calculate loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = correct / total\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")\n",
    "\n",
    "        # Validate after each epoch\n",
    "        validate_model(model, val_loader)\n",
    "\n",
    "# Function to validate our model\n",
    "def validate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Generate a classification report\n",
    "    print(classification_report(all_labels, all_preds, target_names=train_dataset.classes))\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: Train the Model to Obtain Trained Weights\n",
    "\n",
    "In this step, we define the model training parameters and execute the training process to optimize the model's performance.\n",
    "\n",
    "- **Epochs**: Set to **20**, allowing the model to iterate up to 20 times over the dataset to minimize the loss.\n",
    "- **Patience**: Set to **3**, enabling the model to continue training for a maximum of 3 consecutive epochs without improvement in validation loss before stopping early.\n",
    "\n",
    "During training:\n",
    "- For each epoch, a report is generated showing the **training loss**, **training accuracy**, **validation loss**, and **validation accuracy**.\n",
    "- If the validation loss improves, the model's weights are saved.\n",
    "\n",
    "After training:\n",
    "- The model automatically loads the weights corresponding to the best validation performance.\n",
    "\n",
    "This ensures that the training process is efficient, prevents overfitting, and uses early stopping to identify the optimal set of weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 1.4404, Accuracy: 0.5477\n",
      "Validation Loss: 0.9490, Validation Accuracy: 0.7794\n",
      "Validation loss improved. Saving the model to Model_save/Efficientnet_trash_classifier_final.pth.\n",
      "Epoch 2/20, Loss: 0.7612, Accuracy: 0.8006\n",
      "Validation Loss: 0.5038, Validation Accuracy: 0.8578\n",
      "Validation loss improved. Saving the model to Model_save/Efficientnet_trash_classifier_final.pth.\n",
      "Epoch 3/20, Loss: 0.4247, Accuracy: 0.8749\n",
      "Validation Loss: 0.3457, Validation Accuracy: 0.8824\n",
      "Validation loss improved. Saving the model to Model_save/Efficientnet_trash_classifier_final.pth.\n",
      "Epoch 4/20, Loss: 0.2769, Accuracy: 0.9124\n",
      "Validation Loss: 0.2822, Validation Accuracy: 0.9167\n",
      "Validation loss improved. Saving the model to Model_save/Efficientnet_trash_classifier_final.pth.\n",
      "Epoch 5/20, Loss: 0.1998, Accuracy: 0.9466\n",
      "Validation Loss: 0.2145, Validation Accuracy: 0.9363\n",
      "Validation loss improved. Saving the model to Model_save/Efficientnet_trash_classifier_final.pth.\n",
      "Epoch 6/20, Loss: 0.1332, Accuracy: 0.9598\n",
      "Validation Loss: 0.2029, Validation Accuracy: 0.9510\n",
      "Validation loss improved. Saving the model to Model_save/Efficientnet_trash_classifier_final.pth.\n",
      "Epoch 7/20, Loss: 0.1010, Accuracy: 0.9747\n",
      "Validation Loss: 0.1902, Validation Accuracy: 0.9559\n",
      "Validation loss improved. Saving the model to Model_save/Efficientnet_trash_classifier_final.pth.\n",
      "Epoch 8/20, Loss: 0.0796, Accuracy: 0.9774\n",
      "Validation Loss: 0.1878, Validation Accuracy: 0.9461\n",
      "Validation loss improved. Saving the model to Model_save/Efficientnet_trash_classifier_final.pth.\n",
      "Epoch 9/20, Loss: 0.0714, Accuracy: 0.9829\n",
      "Validation Loss: 0.1937, Validation Accuracy: 0.9510\n",
      "No improvement in validation loss. Patience counter: 1/3\n",
      "Epoch 10/20, Loss: 0.0595, Accuracy: 0.9851\n",
      "Validation Loss: 0.1881, Validation Accuracy: 0.9510\n",
      "No improvement in validation loss. Patience counter: 2/3\n",
      "Epoch 11/20, Loss: 0.0469, Accuracy: 0.9868\n",
      "Validation Loss: 0.1747, Validation Accuracy: 0.9510\n",
      "Validation loss improved. Saving the model to Model_save/Efficientnet_trash_classifier_final.pth.\n",
      "Epoch 12/20, Loss: 0.0377, Accuracy: 0.9895\n",
      "Validation Loss: 0.1696, Validation Accuracy: 0.9510\n",
      "Validation loss improved. Saving the model to Model_save/Efficientnet_trash_classifier_final.pth.\n",
      "Epoch 13/20, Loss: 0.0365, Accuracy: 0.9895\n",
      "Validation Loss: 0.1897, Validation Accuracy: 0.9608\n",
      "No improvement in validation loss. Patience counter: 1/3\n",
      "Epoch 14/20, Loss: 0.0415, Accuracy: 0.9873\n",
      "Validation Loss: 0.1873, Validation Accuracy: 0.9412\n",
      "No improvement in validation loss. Patience counter: 2/3\n",
      "Epoch 15/20, Loss: 0.0355, Accuracy: 0.9895\n",
      "Validation Loss: 0.1941, Validation Accuracy: 0.9559\n",
      "No improvement in validation loss. Patience counter: 3/3\n",
      "Early stopping triggered. Stopping training.\n",
      "Loading the best model from Model_save/Efficientnet_trash_classifier_final.pth.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_148121/1995310198.py:62: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "# train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10)\n",
    "model = train_model_with_early_stopping(model, train_loader, val_loader, criterion, optimizer, epochs=20, patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8: Interpreting the Model's Test Evaluation\n",
    "\n",
    "#### Overall Performance\n",
    "- **Accuracy**: 93%\n",
    "- **Precision**: 0.9318\n",
    "- **Recall**: 0.9311\n",
    "- **F1 Score**: 0.9313\n",
    "- **AUC**: 0.9939\n",
    "\n",
    "---\n",
    "\n",
    "### Class-wise Metrics\n",
    "\n",
    "| Class      | Precision | Recall | F1-Score | Support |\n",
    "|------------|-----------|--------|----------|---------|\n",
    "| **Cardboard** | 1.00      | 0.95   | 0.97     | 81      |\n",
    "| **Glass**     | 0.92      | 0.92   | 0.92     | 101     |\n",
    "| **Metal**     | 0.93      | 0.93   | 0.93     | 82      |\n",
    "| **Paper**     | 0.96      | 0.97   | 0.97     | 119     |\n",
    "| **Plastic**   | 0.89      | 0.90   | 0.89     | 97      |\n",
    "| **Trash**     | 0.83      | 0.86   | 0.84     | 28      |\n",
    "\n",
    "---\n",
    "\n",
    "### Key Insights\n",
    "1. **Strengths**:\n",
    "   - The model performs exceptionally well across most categories, especially \"Cardboard,\" \"Glass,\" \"Metal,\" and \"Paper.\"\n",
    "   - High AUC (0.9939) demonstrates strong class separation.\n",
    "   - Decently high accuracy of ~93%\n",
    "\n",
    "2. **Weaknesses**:\n",
    "   - Lower performance on \"Trash\" (F1-score: 0.84), likely due to limited examples and variability.\n",
    "   - Slightly lower F1-score for \"Plastic\" (0.89) indicates room for improvement.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_162476/78138125.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('Model_save/Efficientnet_trash_classifier_final.pth'))  # Load the weights\n",
      "[codecarbon INFO @ 00:06:42] Energy consumed for RAM : 0.000139 kWh. RAM Power : 5.572976589202881 W\n",
      "[codecarbon INFO @ 00:06:42] Energy consumed for all CPUs : 0.001063 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:06:42] 0.001202 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:06:57] Energy consumed for RAM : 0.000163 kWh. RAM Power : 5.572976589202881 W\n",
      "[codecarbon INFO @ 00:06:57] Energy consumed for all CPUs : 0.001240 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:06:57] 0.001402 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   cardboard       1.00      0.95      0.97        81\n",
      "       glass       0.92      0.92      0.92       101\n",
      "       metal       0.93      0.93      0.93        82\n",
      "       paper       0.96      0.97      0.97       119\n",
      "     plastic       0.89      0.90      0.89        97\n",
      "       trash       0.83      0.86      0.84        28\n",
      "\n",
      "    accuracy                           0.93       508\n",
      "   macro avg       0.92      0.92      0.92       508\n",
      "weighted avg       0.93      0.93      0.93       508\n",
      "\n",
      "Precision: 0.9318\n",
      "Recall: 0.9311\n",
      "F1 Score: 0.9313\n",
      "AUC: 0.9939\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(0.9318254002345704),\n",
       " np.float64(0.9311023622047244),\n",
       " np.float64(0.9313297124885986),\n",
       " np.float64(0.9939290936080889),\n",
       " '              precision    recall  f1-score   support\\n\\n   cardboard       1.00      0.95      0.97        81\\n       glass       0.92      0.92      0.92       101\\n       metal       0.93      0.93      0.93        82\\n       paper       0.96      0.97      0.97       119\\n     plastic       0.89      0.90      0.89        97\\n       trash       0.83      0.86      0.84        28\\n\\n    accuracy                           0.93       508\\n   macro avg       0.92      0.92      0.92       508\\nweighted avg       0.93      0.93      0.93       508\\n')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 00:07:12] Energy consumed for RAM : 0.000186 kWh. RAM Power : 5.572976589202881 W\n",
      "[codecarbon INFO @ 00:07:12] Energy consumed for all CPUs : 0.001417 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:07:12] 0.001602 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:07:12] 0.004928 g.CO2eq/s mean an estimation of 155.4245696592646 kg.CO2eq/year\n",
      "[codecarbon INFO @ 00:07:27] Energy consumed for RAM : 0.000209 kWh. RAM Power : 5.572976589202881 W\n",
      "[codecarbon INFO @ 00:07:27] Energy consumed for all CPUs : 0.001594 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:07:27] 0.001803 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:07:42] Energy consumed for RAM : 0.000232 kWh. RAM Power : 5.572976589202881 W\n",
      "[codecarbon INFO @ 00:07:42] Energy consumed for all CPUs : 0.001771 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:07:42] 0.002003 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:07:57] Energy consumed for RAM : 0.000255 kWh. RAM Power : 5.572976589202881 W\n",
      "[codecarbon INFO @ 00:07:57] Energy consumed for all CPUs : 0.001948 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:07:57] 0.002203 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:08:12] Energy consumed for RAM : 0.000279 kWh. RAM Power : 5.572976589202881 W\n",
      "[codecarbon INFO @ 00:08:12] Energy consumed for all CPUs : 0.002125 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:08:12] 0.002403 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test set\n",
    "model = efficientnet_b0(num_classes=6)  # Adjust num_classes as per your task\n",
    "model.load_state_dict(torch.load('Model_save/Efficientnet_trash_classifier_final.pth'))  # Load the weights\n",
    "model.to(device)  # Move to the appropriate device (CPU/GPU)\n",
    "model.eval()  # Set to evaluation mode\n",
    "validate_model_with_metrics_and_report(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 9: Finish Code-Carbon Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 00:08:15] Energy consumed for RAM : 0.000282 kWh. RAM Power : 5.572976589202881 W\n",
      "[codecarbon INFO @ 00:08:15] Energy consumed for all CPUs : 0.002151 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:08:15] 0.002433 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CO2 emissions: 0.90 g\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/platinumfish/miniconda3/envs/Climate_CNN_3.10/lib/python3.10/site-packages/codecarbon/output_methods/file.py:52: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame.from_records([dict(total.values)])])\n"
     ]
    }
   ],
   "source": [
    "emissions = tracker.stop()\n",
    "emissions_in_grams = emissions * 1000  # Convert kg to grams\n",
    "print(f\"Total CO2 emissions: {emissions_in_grams:.2f} g\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Climate_CNN_3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
